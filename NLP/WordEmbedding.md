# Word Embeddings

mapping words/phrases from a vocabulary into a vectors of real numberrs

Word2Vec:
GloVe
fastText 
gensim
t-SNE: t-distributed stochastic Neighbour Embedding


## Word2Vec - Google
shallow two-layer neural network
input : large corpus of text 
output: vector space (hundred dimension)
with each unique word in the copus being assigned a corresponding vector in the space

word vectors are positioned in the vector space such that words that share common contexts in the corpus are located in cclose proximity to one another in the space

## GloVe
Global Vectors 
a model for distributed word representation
the model is an unsupervised learning algo for obtaining vector representations for words
this is achieved by mapping words into a meaningful space where the distance between words is related to semantic similarity


## fastText - Facebook
The model allows to create an unsupervised learning or supervised learning algorithm for obtaining vector representations for words.

## Gensim - 2009
